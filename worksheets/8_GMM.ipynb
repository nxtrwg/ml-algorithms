{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main aims of this tutorial are (i) to get experience in fitting Gaussian mixture models using *sklearn* module, and to (ii) assess several methods for choosing the number of clusters. Furthermore, we'll see how a fitted model allows us to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with same old initialisation line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a Gaussian mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate data originated from several Gaussians. We will need quite a few parameters to generate such a dataset. To begin with we need to decide how many components (clusters) to generate, for example we can start with $3$ clusters. For each cluster, we need to decide how many points to sample. We can keep these numbers in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = np.array([30, 20, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to use 2D data for visualisation purposes. Thus each Gaussian is defined by a 2D mean vector and a $2\\times2$ covariance matrix. Let's collate all mean information into one array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u1 = np.array([0, 0]) # mean of the 1st component\n",
    "u2 = np.array([50, 60]) # mean of the 2nd component\n",
    "u3 = np.array([0, 100]) # mean of the 3rd component\n",
    "u = np.vstack((u1, u2, u3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance matrices have to by symmetric. Hence there are only $3$ parameters incurred by each matrix. For each Gaussian, suppose we keep covariance matrix values in an array, such that the first element is the variance along the x-axis, the second element is the variance along the y-axis, and the last element is covariance. Again, let's collate all variance/covariance information into a single array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1 = np.array([160, 180, 20])\n",
    "v2 = np.array([170, 120, 30])\n",
    "v3 = np.array([130, 130, 40])\n",
    "v = np.vstack((v1, v2, v3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement a function that will generate us a random sample from a Gaussian mixture defined by parameters $n$, $u$, and $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_data(n, u, v):\n",
    "    # ... your code here\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = generate_data(n, u, v)\n",
    "plt.plot(data[:,0], data[:,1], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Gaussian mixture model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have data, let's fit a Gaussian mixture model (GMM). This can be done just in a couple of lines of code using *sklearn*. Note that the fitting procedure implements expectation maximisation algorithm. We have to specify the number of clusters, and at first let's specify the true number. The *covariance_type* parameter allows one to make fitting more efficient by, e.g., restricting Gaussians to spherical shapes. In our case, we do not impose any additional restrictions on the covariance matrices, and hence use the *full* option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "gmix = mixture.GMM(n_components=3, covariance_type='full')\n",
    "gmix.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore the estimated parameters of the fitted model. Parameters include weights, means and covariance matrices for each Gaussian component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(gmix.weights_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(gmix.means_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(gmix.covars_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a 2D data, we can also visualise the fitted model. The 2D Gaussians can be represented with isoline ellipsoids. For each Gaussian component, the ellipsoid is a location of points that have the same probability. Plotting an ellipsoid for a given 2D Gaussian, is somewhat non-trivial, and we are going to use a function developed for this purpose. Understanding the code and theory of function *plot_cov_ellipse* is not necessary for this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted from http://www.nhsilbert.net/source/2014/06/bivariate-normal-ellipse-plotting-in-python/\n",
    "# and https://github.com/joferkington/oost_paper_code/blob/master/error_ellipse.py\n",
    "def plot_cov_ellipse(cov, pos, nstd=2, ax=None, fc='none', ec=[0,0,0], a=1, lw=2):\n",
    "    \"\"\"\n",
    "    Plots an `nstd` sigma error ellipse based on the specified covariance\n",
    "    matrix (`cov`). Additional keyword arguments are passed on to the \n",
    "    ellipse patch artist.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        cov : The 2x2 covariance matrix to base the ellipse on\n",
    "        pos : The location of the center of the ellipse. Expects a 2-element\n",
    "            sequence of [x0, y0].\n",
    "        nstd : The radius of the ellipse in numbers of standard deviations.\n",
    "            Defaults to 2 standard deviations.\n",
    "        ax : The axis that the ellipse will be plotted on. Defaults to the \n",
    "            current axis.\n",
    "        Additional keyword arguments are pass on to the ellipse patch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A matplotlib ellipse artist\n",
    "    \"\"\"\n",
    "    from scipy.stats import chi2\n",
    "    from matplotlib.patches import Ellipse\n",
    "    \n",
    "    def eigsorted(cov):\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        return vals[order], vecs[:,order]\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "    \n",
    "    kwrg = {'facecolor':fc, 'edgecolor':ec, 'alpha':a, 'linewidth':lw}\n",
    "\n",
    "    # Width and height are \"full\" widths, not radius\n",
    "    width, height = 2 * nstd * np.sqrt(vals)\n",
    "    ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, **kwrg)\n",
    "\n",
    "    ax.add_artist(ellip)\n",
    "    return ellip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above function, implement visualisation that plots data overlaid with fitted Gaussian ellipsoids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def plot_gmm(data, gmm):\n",
    "    # ... your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_gmm(data, gmix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiment with fitting to different datasets, including samples with overlapping clusters*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of a probabilistic model is in the ability to make predictions. That is, for each point, either from the dataset or a new point, we can now assess the probabilities that the point originated from each of the components. We can then assign the point to the most probable component (cluster). In other words, we can predict the cluster for the point, and this can be done using a standard function provided within the *mixture* module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's cluster each point from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c=gmix.predict(data), lw=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, generate a gird of regularly spaced points and see how the entire space is divided into clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delta = 1\n",
    "\n",
    "xMin = np.round(np.min(data[:,0])) - 5\n",
    "xMax = np.round(np.max(data[:,0])) + 5\n",
    "yMin = np.round(np.min(data[:,1])) - 5\n",
    "yMax = np.round(np.max(data[:,1])) + 5\n",
    "\n",
    "xSpace = np.arange(xMin, xMax, delta)\n",
    "ySpace = np.arange(yMin, yMax, delta)\n",
    "xGrid, yGrid = np.meshgrid(xSpace, ySpace)\n",
    "\n",
    "newData = transpose(np.stack((np.ravel(xGrid), np.ravel(yGrid))))\n",
    "print(newData.shape)\n",
    "\n",
    "plt.scatter(newData[:,0], newData[:,1], c=gmix.predict(newData), lw=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the wrong number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would happen if we specify a wrong number of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmix2 = mixture.GMM(n_components=2, covariance_type='full')\n",
    "gmix2.fit(data)\n",
    "plot_gmm(data, gmix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmix4 = mixture.GMM(n_components=5, covariance_type='full')\n",
    "gmix4.fit(data)\n",
    "plot_gmm(data, gmix4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What can you conclude from this exercise?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the tutorial, let's generate a more complicated dataset with a larger number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... your code here\n",
    "data = generate_data(n, u, v)\n",
    "plt.plot(data[:,0], data[:,1], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, we usually do not know the true number of clusters, so let's try to estimate it solely from the data. We are going to try a few different approaches, including heldout cross validation log-likelihood , AIC and BIC. Before we begin, generate heldout data from the same distribution as original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = np.array([30, 20, 10, 25, 25])\n",
    "heldOutData = generate_data(n, u, v)\n",
    "plt.plot(data[:,0], data[:,1],'.', label= 'train-data')\n",
    "plt.plot(heldOutData[:,0], heldOutData[:,1],'*',label='holdout-data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit Gaussian mixture models to the data using various cluster numbers. After each fit, you will be able to access a few quantities that we are going to use. If *gmix* is a fitted model, then *gmix.score(X)* returns log-likelihood for each point in *X*, *gmix.aic* returns the AIC, and *gmix.bic* returns the BIC for the fitted model. Note that in practice it is recommended to use AIC corrected for small sample sizes. The corrected version is defined as $AICc=AIC+2n_{par}(n_{par}+1)/(n-n_{par}-1)$, where $n_{par}$ is the number of free parameters, and $n$ is the number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit Gaussian mixture models to the data using various cluster numbers, and for each number, record log-likelihood, cross validated log-likelihood, AIC, AICc, BIC and number of free parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kRange = range(1,10)\n",
    "nk = len(kRange)\n",
    "ll = np.zeros(nk)\n",
    "ll2 = np.zeros(nk)\n",
    "aic = np.zeros(nk)\n",
    "aicc = np.zeros(nk)\n",
    "bic = np.zeros(nk)\n",
    "nParams = np.zeros(nk) # number of free parameters\n",
    "nSamples = data.shape[0] # sample size\n",
    "\n",
    "for i in range(0, nk):\n",
    "    # ... your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot all these quantities as a function of hypothesised number of clusters. *Analyse the resulting plots. What can you tell about the number of parameters? Can all of these quantities be used to estimate the number of clusters?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logl_data = plt.plot(kRange, ll, 'b.-',label = 'Data log-likelihood')\n",
    "logl_dataholdout = plt.plot(kRange, ll2, 'k.-', label = 'HoldOut log-likelihood')\n",
    "#plt.legend()\n",
    "plt.xlabel('k - # of components')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=2, mode=\"expand\", borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(kRange, nParams, 'c.-', label = '# of Parameters')\n",
    "plt.plot(kRange, np.repeat(nSamples, len(kRange)), 'y.-', label = '# of Samples')\n",
    "plt.xlabel('k - # of components')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=2, mode=\"expand\", borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(kRange, aic, 'r.-', label = 'AIC')\n",
    "plt.plot(kRange, aicc, 'm.-',label = 'AICc')\n",
    "plt.plot(kRange, bic, 'g.-', label = 'BIC')\n",
    "plt.xlabel('k - # of components')\n",
    "plt.legend()\n",
    "#plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=3, mode=\"expand\", borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
