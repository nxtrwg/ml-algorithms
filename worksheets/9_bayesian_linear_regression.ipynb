{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop 8: Bayesian Inference with Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* Chrome displays a vertical bar after all maths expressions. This is not intentional, it's a display bug. If this annoys you, please use a different browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop we will be looking at Bayesian linear regression. Briefly, this involves learning a linear regression model from a training set of *(x, y)* pairs, where *x* is a vector representing a data point and *y* is a real-valued response variable. Earlier we looked at ridge regression, which involved:\n",
    "1. assuming a linear relationship between inputs and outputs, i.e., $y \\approx \\mathbf{x}' \\mathbf{w}$ for all pairs\n",
    "1. minimising the *residual sum of squares error*, that is finding the parameters $\\hat{\\mathbf{w}}$ that give the best fit to the training responses (subject to added *regularisation* term) \n",
    "1. using $\\hat{\\mathbf{w}}$ to make test inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will look at *Bayesian* inference for the above model. In this case we don't follow steps 2 and 3 above, but rather formulate the *posterior* over the parameters, and make test inferences using *all settings of the parameters* weighted by their posterior probability. These operations can be solved exactly, using linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Desperately) seeking posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the implementation, let's recap the maths behind Bayesian linear regression. We assume the data was generated from a Normal distribution, with its mean a linear function of the input vector and constant variance $\\sigma^2$ (assumed known, herein).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\newcommand\\ys{\\mathbf{y}}\n",
    "\\newcommand\\xs{\\mathbf{x}}\n",
    "\\newcommand\\Xs{\\mathbf{X}}\n",
    "\\newcommand\\ws{\\mathbf{w}}\n",
    "\\newcommand\\Vs{\\mathbf{V}}\n",
    "\\newcommand\\Is{\\mathbf{I}}\n",
    "\\begin{align*}\n",
    "y &\\sim \\textrm{Normal}(\\xs' \\ws, \\sigma^2) & \\mbox{Likelihood}\\\\\n",
    "\\ws &\\sim \\textrm{Normal}(\\mathbf{0}, \\gamma^2 \\mathbf{I}_D) & \\mbox{Prior}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior over the weights encourages low-magnitude weights, with parameter $\\gamma^2$ controlling the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this formulation, we next step is to find an expression for the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "p(\\ws | \\Xs, \\ys, \\sigma^2) = \\frac{ p(\\ys | \\Xs, \\ws, \\sigma^2) p(\\ws) }{ p(\\ys | \\Xs) } \\propto p(\\ys | \\Xs, \\ws, \\sigma^2) p(\\ws)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\Xs$ are the training points (a matrix) and $\\ys$ are a vector of the response values for each training point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that this can be solved, resulting in a Normal distribution for the posterior. We'll come back to this later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll generate a simple synthetic dataset with a tiny handful of points drawn from a simple quadratic function,\n",
    "$$ y = 5(x-\\frac{1}{2})^2 $$\n",
    "with a small amount of added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18615756]\n",
      " [ 0.59675711]\n",
      " [ 0.67832224]\n",
      " [ 0.70492722]\n",
      " [ 0.89709031]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x84b2860>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x84b2748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate a few points\n",
    "N = 5\n",
    "# each x is a scalar, uniformly drawn between [0, 1] \n",
    "X = np.sort(np.random.random(size=N))\n",
    "# reshape X into a matrix, such that each instance is a row\n",
    "X = X[:,np.newaxis]\n",
    "\n",
    "#Fixed Run#3 for more clear uncertainty plot\n",
    "#X2 = [ [0.18615756], [0.59675711] ,[0.67832224] ,[0.70492722] ,[0.89709031]]\n",
    "#X = np.array(X2)\n",
    "\n",
    "print(X)\n",
    "\n",
    "# generate the target response values using the quadratic function\n",
    "# and additive noise\n",
    "sigma2 = 0.1**2\n",
    "y = 5*(X-0.5)**2 + sigma2**0.5 * np.random.normal(size=(N,1))\n",
    "\n",
    "# plot the training data\n",
    "plt.plot(X, y, 'bo')\n",
    "\n",
    "# and plot the true function (without noise)\n",
    "Xp = np.arange(-0.2, 1.2, 0.01)\n",
    "Xp = Xp[:,np.newaxis]\n",
    "yp_gold = 5*(Xp-0.5)**2 \n",
    "plt.plot(Xp, yp_gold, 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit this data with a polynomial model, considering a few different orders. Recall the basis function trick whereby you can augment the features of the input with new columns to implement a richer model class. Here we'll create a *basis matrix* for a $d^{th}$ order polynomial as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Phi = \\begin{bmatrix}\n",
    "    1 & x_1 & x_1^2 & x_1^3 & \\ldots & x_1^d \\\\\n",
    "    1 & x_2 & x_2^2 & x_2^3 & \\ldots & x_2^d \\\\\n",
    "    \\vdots &    \\vdots &    \\vdots &\\vdots &    \\ddots & \\vdots \\\\\n",
    "    1 & x_n & x_n^2 & x_n^3 & \\ldots & x_n^d \\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how each row is a training point raised to a different power, from *0* to *d*. This means that when we take a row from $\\Phi$ and take the dot product with $\\mathbf{w}$, a parameter vector of size *d+1*, we obtain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{align}\n",
    "\\Phi_{i} \\mathbf{w} &= \\sum_{j=0}^d \\Phi_{ij} w_j \\\\\n",
    "&= 1 \\times w_0 + x_i \\times w_1 + x_i^2 \\times w_2 + \\ldots + x_i^d \\times w_d\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I.e., a $d^{th}$ order polynomial expression with $\\mathbf{w}$ giving the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polynomial_basis(X, Xtest, d):\n",
    "    # ... Over to you\n",
    "    return Phi, Phi_test\n",
    "\n",
    "Phi, Phi_p = polynomial_basis(X, Xp, 9)\n",
    "print (Phi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: How does this basis trick relate to kernel methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the maths, the next step is to formulate the posterior. We plug in the Normal defintions of the likelihood and prior to solve for the unnormalised posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "p(\\ws | \\Xs, \\ys, \\sigma^2) & \\propto \\textrm{Normal}(\\ws | \\mathbf{0}, \\gamma^2 \\mathbf{I}_D)\n",
    "\\textrm{Normal}(\\ys | \\Xs \\ws, \\sigma^2 \\mathbf{I}_N) \\\\\n",
    "& \\propto  \\textrm{Normal}(\\ws |  \\ws_N, \\mathbf{V}_N) \\\\\n",
    "\\mbox{where} ~ \\ws_N &= \\frac{1}{\\sigma^2} \\Vs_N \\Phi' \\ys \\\\\n",
    "\\Vs_N &= \\sigma^2 ( \\Phi' \\Phi + \\frac{\\sigma^2}{\\gamma^2} \\Is_D )^{-1} \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is (proportional to) a Normal distribution. As we know the normalising term for a Gaussian [the denominator of the PDF](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Non-degenerate_case), this means we have an exact normalised solution for the posterior, i.e.,  $p(\\ws | \\Xs, \\ys, \\sigma^2) = \\textrm{Normal}(\\ws |  \\ws_N, \\mathbf{V}_N)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the mean vector and covariance matrix for the posterior based on the above expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma2 = 100 # large number = more permissive, prior is broader\n",
    "VN = # ... Over to you\n",
    "wN = # ... Over to you\n",
    "print(wN.shape, VN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can plot the prior and posterior to see how they differ; given we have several weights, we'll just look at a couple of these, $w_0$ and $w_1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up a 2d plot mesh\n",
    "plt.figure(1)\n",
    "delta = 0.05\n",
    "xpl = np.arange(-3, 3, delta)\n",
    "ypl = np.arange(-3, 3, delta)\n",
    "Xpl, Ypl = np.meshgrid(xpl, ypl)\n",
    "\n",
    "# plot a bivariate normal for the prior\n",
    "Zpl = mlab.bivariate_normal(Xpl, Ypl, gamma2 ** 0.5,gamma2 ** 0.5, 0, 0, 0)\n",
    "CS = plt.contour(Xpl, Ypl, Zpl) \n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "plt.title('Prior p(w) indices 0,1')\n",
    "\n",
    "# plot a bivariate normal for the posterior\n",
    "plt.figure(2)\n",
    "Zpl = mlab.bivariate_normal(Xpl, Ypl, VN[0,0] ** 0.5, VN[1,1] ** 0.5, wN[0], wN[1], VN[0,1])\n",
    "CS = plt.contour(Xpl, Ypl, Zpl) #, levels=[0.001, 0.1, 0.25, 0.5, 0.75, 1, 1.5, 2, 2.5])\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "plt.title('Posterior p(w|X,y) indices 0,1')\n",
    "plt.plot(wN[0], wN[1], 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion question**: Can you explain why the prior and the posterior are so different? How is this related to the dataset? Why are the elipses in the posterior not aligned to the axes? *You might want to change the parameter indices from 0,1 to other pairs to get a better idea of the full posterior.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conceptually simplest means of doing inference is to draw a few parameter vectors from the posterior (sampling from a Gaussian). Let's do this 10 times and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the data points\n",
    "plt.plot(X, y, 'o')\n",
    "# plot the mean prediction\n",
    "yp = np.dot(Phi_p, wN)\n",
    "plt.plot(Xp, yp, 'g-')\n",
    "\n",
    "# generate some samples from the posterior\n",
    "samples = []\n",
    "ps = []\n",
    "for s in range(10):\n",
    "    # draw a weight vector\n",
    "    w_sampled = np.random.multivariate_normal(wN.flatten(), VN, 1).flatten()\n",
    "    # plot the predictions for this weight vector\n",
    "    yp_sampled = np.dot(Phi_p, w_sampled)\n",
    "    p = plt.plot(Xp.flatten(), yp_sampled.flatten(), ':', lw=3)\n",
    "    samples.append(w_sampled)\n",
    "    ps.append(p[0])\n",
    "    \n",
    "plt.ylim(-1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to see what happens near the data points, and away from them, in particular the edges of the plot. We'll come back to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there's a more elegant solution, as the predictive distribution can be found in closed form. Namely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "p(y_{*} | \\xs_{*}, \\Xs, \\ys, \\sigma^2) &= \\textrm{Normal}(y_{*} | \\xs_{*}'\\ws_N, \\sigma^2_N(\\xs_{*})) \\\\\n",
    "\\sigma^2_N(\\xs_{*}) & = \\sigma^2 + \\xs_{*}' \\Vs_N \\xs_{*}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the predictive mean is a simple application of the posterior mean to the data point, but the predictive variance is  a bit more complicated. Let's compute the mean and variance on the test points, and visualise the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the mean from above\n",
    "yp = # ... Over to you\n",
    "\n",
    "# compute the variance using formula above\n",
    "s2s = np.zeros(Xp.shape[0])\n",
    "for i in range(Xp.shape[0]):\n",
    "    s2s[i] = # ... Over to you\n",
    "\n",
    "# plot the data\n",
    "plt.plot(X, y, 'o')\n",
    "# plot the 95% confidence interval\n",
    "plt.fill_between(Xp.flatten(), yp+2*s2s ** 0.5, yp-2*s2s ** 0.5, facecolor='blue', alpha=0.1)\n",
    "# plot the mean as a green dotted line\n",
    "plt.plot(Xp, yp, 'g:')\n",
    "# plot the ground truth in red\n",
    "plt.plot(Xp, yp_gold, 'r')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.ylim(-2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: How does the uncertainty plot compare to the samples above? How does the uncertainty change relative to the distance from training points? Can you explain why?\n",
    "\n",
    "**Practical**: How does the setting of *gamma2* affect the fit? How about the number of data points, *N*. Try some other values and see what happens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: Is a 9th order polynomial a good choice for this problem? Based on the results above, would you recommend this model, or make a different choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's revisit the assumption of having a 9th order polynomial. The evidence gives us a good way of evaluating the quality of the fit. \n",
    "\n",
    "We can compute the evidence, $p(\\ys|\\Xs, \\sigma^2, \\gamma^2)$ <s>based on slide 31 of deck 14 (note the notation key, for mapping between Bishop to Murphy notation)</s>. This is also known as the *marginal likelihood* and is given in Bishop 3.5.1, p167 as equation 3.86. We won't delve into the details, but broadly speaking the $|\\mathbf{A}|$ term penalises model complexity, while the $E$ term ($E(\\mathbf{m}_N)$ in Bishop) measures the quality of the fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_evidence(m, Phi, sigma2, gamma2):\n",
    "    N, M = Phi.shape\n",
    "    alpha, beta = 1/float(gamma2), 1/float(sigma2)\n",
    "    rss = np.sum((y - np.dot(Phi, m)) ** 2)\n",
    "    wpen = np.dot(m.T, m)\n",
    "    E = beta/2.0 * rss + alpha/2.0 * wpen\n",
    "    A = alpha * np.eye(M) + beta * np.dot(Phi.T, Phi)\n",
    "    lE = M/2.0 * np.log(alpha) + N/2.0 * np.log(beta) - E \\\n",
    "        - 0.5 * np.log(np.linalg.det(A)) - N/2.0 * np.log(2.0 * np.pi)\n",
    "\n",
    "    # return both the evidence, and the RSS term (the raw quality of fit)\n",
    "    return lE.flatten()[0], rss\n",
    "\n",
    "# what's the evidence for our 9th order model?\n",
    "print(log_evidence(wN, Phi, sigma2, gamma2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what happens if we use a lower order model, e.g., a 3rd order model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Phi3, Phi_p3 = polynomial_basis(X, Xp, 3)\n",
    "VN3 = # ... Copy from above\n",
    "wN3 = # ... Copy from above\n",
    "\n",
    "yp3 = # ... Copy from above\n",
    "s2s = np.zeros(Phi_p3.shape[0])\n",
    "for i in range(Phi_p3.shape[0]):\n",
    "    \n",
    "    s2s[i] = # ... Copy from above\n",
    "    \n",
    "plt.plot(X, y, 'o')\n",
    "plt.fill_between(Xp.flatten(), yp3+2*s2s ** 0.5, yp3-2*s2s ** 0.5, facecolor='blue', alpha=0.1)\n",
    "plt.plot(Xp, yp3, 'g:')\n",
    "plt.plot(Xp, yp_gold, 'r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: does that look like a better fit to you? Consider both the area [0,1] near the training points, and those outside this range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the *evidence* says, and compare this to the above result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... Over to you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RSS has barely changed, but the evidence is much higher. We can look at various model orders to see which has the best *evidence* to perform Bayesian model selection: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evs = []\n",
    "gamma2 = 1000\n",
    "for order in range(0,10):\n",
    "    Phio, Phipo = polynomial_basis(X, Xp, order)\n",
    "    Vo = # ... Copy from above\n",
    "    wo = # ... Copy from above\n",
    "    lEo = # ... Copy from above\n",
    "    print('order', order, 'log evidence', lEo[0], 'rss', lEo[1])\n",
    "    evs.append((order,) + lEo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot the above log evidence values against the model order\n",
    "evs = np.vstack(evs)\n",
    "plt.plot(evs[:,0], evs[:,1])\n",
    "#plt.ylim(-100, -5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: So which model class will be chosen? Is this a reasonable situation? \n",
    "\n",
    "**Practical**: Rerun the code with a new random training set, or different values of *N*, such as 2 or 3 points or 20; the results may be different. Can you explain why the outcome might be different?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
